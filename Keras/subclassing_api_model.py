# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mZuraXVlOx-r65Vhda0-3VAOXK3xppAg

#Subclassing api ile dinamik model oluşturabiliriz
1.diğer yöntemlere göre daha zordur.

2.döngüler, koşullu dallanmalar gibi şeyleri içerebilir biz kurduğumuzdan.
"""

import tensorflow as tf
from tensorflow import keras

#lineer bir dense katmanı oluşturuyoruz.
class Linear(keras.layers.Layer):
  def __init__(self,units=32,input_dim=32):
    super(Linear,self).__init__()
    #şimdi ağırlık niteliği oluşturucaz
    w_init=tf.random_normal_initializer()
    #ağırlık niteliği fonksiyonu
    self.w=tf.Variable(
        initial_value=w_init(shape=(input_dim,units),dtype="float32"),
        trainable=True #ağırlıkların eğitilebiliceğini ifade eder geri dönüş yöntemiyle mesela

    )
    #şimdi de bias niteliğini oluşturuyoruz.
    b_init=tf.zeros_initializer()
    #bias biteliği fonksiyonu
    self.b=tf.Variable(
        initial_value=b_init(shape=(units,),dtype="float32"),
        trainable=True
    )

  def call(self,inputs):
    return tf.matmul(inputs,self.w)+self.b

x=tf.ones((2,2))
x

linear_layer=Linear(4,2) # 4 nöronu ve girdi boyutu da 2
y=linear_layer(x)
print(y)

"""Girdi boyutunu bilmeden, yazmadan katman ekleme"""

class Linear(keras.layers.Layer):
    def __init__(self, units=32):
        super(Linear, self).__init__()
        self.units = units

    def build(self, input_shape):
        self.w = self.add_weight(
            shape=(input_shape[-1], self.units),
            initializer="random_normal",
            trainable=True,
        )
        self.b = self.add_weight(
            shape=(self.units,),
            initializer="random_normal",
            trainable=True,
        )

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b

linear_layer=Linear(32,)
y=linear_layer(x)
y

"""Art arda gelen katmanlar"""

class MLPBlock(tf.keras.layers.Layer):
  def __init__(self):
    super(MLPBlock,self).__init__()
    #girdi girmeye gerek yok çünkü bunlar ara katman
    self.linear_1=Linear(32) # ara katman 1
    self.linear_2=Linear(32) #ara katman 2
    self.linear_3=Linear(1) #output katmanı

  def call(self,inputs):
    x=self.linear_1(inputs) # ara katmana gidiyor ve relu aktivasyonundan geçiyor
    x=tf.nn.relu(x)

    x=self.linear_2(x)
    x=tf.nn.relu(x)

    return self.linear_3(x) #en son outputa gidiyor

mlp=MLPBlock()
y=mlp(tf.ones(shape=(3,64)))

mlp.weights

"""#MODEL OLUŞTURMA"""

from sklearn.datasets import fetch_california_housing
housing=fetch_california_housing()

for i in housing:
  print(i)

from sklearn.model_selection import train_test_split

X_train_full,X_test,y_train_full,y_test=train_test_split(housing.data,housing.target,random_state=42)

X_train,X_valid,y_train,y_valid=train_test_split(X_train_full,y_train_full,random_state=42)

class WideAndDeepModel(tf.keras.Model):
  def __init__(self,units=30,activation="relu",**kwargs):
    super().__init__(**kwargs)
    self.norm_layer_wide=keras.layers.Normalization()
    self.norm_layer_deep=keras.layers.Normalization()
    self.hidden1=keras.layers.Dense(units,activation=activation)
    self.hidden2=keras.layers.Dense(units,activation=activation)
    self.main_output=keras.layers.Dense(1)
  def call(self,inputs):
    input_wide,input_deep=inputs
    norm_wide=self.norm_layer_wide(input_wide)
    norm_deep=self.norm_layer_deep(input_deep)
    hidden1=self.hidden1(norm_deep)
    hidden2=self.hidden2(hidden1)
    concat=keras.layers.concatenate([norm_wide,hidden2])
    output=self.main_output(concat)
    return output

tf.random.set_seed(42)
model=WideAndDeepModel(
    30,
    activation="relu",
    name="my_model"
)

optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(loss="mse",optimizer=optimizer,metrics=["RootMeanSquaredError"])

X_train_wide,X_train_deep=X_train[:,:5],X_train[:,2:]
X_valid_wide,X_valid_deep=X_valid[:,:5],X_valid[:,2:]
X_test_wide,X_test_deep=X_test[:,:5],X_test[:,2:]

model.norm_layer_wide.adapt(X_train_wide)
model.norm_layer_deep.adapt(X_train_deep)

history=model.fit((X_train_wide,X_train_deep),y_train,epochs=20,validation_data=((X_valid_wide,X_valid_deep),y_valid))

eval=model.evaluate((X_test_wide,X_test_deep),y_test)
eval

X_new_wide,X_new_deep=X_test_wide[:3],X_test_deep[:3]
y_pred=model.predict((X_new_wide,X_new_deep))
y_pred

y_test[:3]